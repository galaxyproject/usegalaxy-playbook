---

## these vars are defined in vault.yml
#
# used by: galaxyproject.galaxy (templating job_conf.yml)
galaxy_job_conf_amqp_url: "amqp://{{ pulsar_amqp_credentials }}@{{ amqp_server }}:5671//test_pulsar?ssl=1"
# For single-Pulsar dests like rockfish-devgalaxy
galaxy_job_conf_main_amqp_url: "{{ vault_galaxy_job_conf_main_amqp_url }}"

# used by: oidc backends config template
galaxy_custos_client_id: "{{ vault_galaxy_custos_client_id }}"
galaxy_custos_client_secret: "{{ vault_galaxy_custos_client_secret }}"
galaxy_google_client_id: "{{ vault_galaxy_google_client_id }}"
galaxy_google_client_secret: "{{ vault_galaxy_google_client_secret }}"

# file_sources_conf.yml
covid_crg_ftp_staging_user: "{{ vault_covid_crg_ftp_staging_user }}"
covid_crg_ftp_staging_passwd: "{{ vault_covid_crg_ftp_staging_passwd }}"

genomeark_galaxy_aws_secret_access_key: "{{ vault_genomeark_galaxy_aws_secret_access_key }}"
genomeark_galaxy_aws_access_key_id: "{{ vault_genomeark_galaxy_aws_access_key_id }}"
genomeark_vgl_aws_secret_access_key: "{{ vault_genomeark_vgl_aws_secret_access_key }}"
genomeark_vgl_aws_access_key_id: "{{ vault_genomeark_vgl_aws_access_key_id }}"
paratus_aws_secret_access_key: "{{ vault_paratus_aws_secret_access_key }}"
paratus_aws_access_key_id: "{{ vault_paratus_aws_access_key_id }}"
elementbio_aws_secret_access_key: "{{ vault_elementbio_aws_secret_access_key }}"
elementbio_aws_access_key_id: "{{ vault_elementbio_aws_access_key_id }}"
galaxy_sandbox_access_key_id: "{{ vault_galaxy_sandbox_access_key_id }}"
galaxy_sandbox_secret_access_key: "{{ vault_galaxy_sandbox_secret_access_key }}"

# used by interactive_tool_chat_analysis
groq_api_key: "{{ vault_groq_api_key }}"

# Hashicorp Vault token
galaxy_vault_token: "{{ vault_galaxy_vault_token }}"

webhook_links:
  - gtn

webhook_plugins:
  - toolmsg

## used by: galaxy supervisor templates
galaxy_host_codename: test

# FIXME: "{{ galaxy_remote_users.privsep | default(omit) }}" is blanking remote_user on the play. I set the below option
# to workaround but this needs to be fixed in galaxyproject.galaxy, nothing should be breaking when privsep mode is not
# enabled in the role
galaxy_remote_users:
  privsep: "{{ galaxy_privileged_user }}"
  errdocs: "{{ galaxy_privileged_user }}"
  galaxy: "{{ galaxy_user }}"
  #root: gxadm

galaxy_become_users: {}
  #privsep: "{{ galaxy_privileged_user }}"
  #errdocs: "{{ galaxy_privileged_user }}"
  #galaxy: "{{ galaxy_user }}"
  #root: root


galaxy_server_dir: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
galaxy_shed_tools_dir: /cvmfs/{{ galaxy_cvmfs_repo }}/shed_tools
#galaxy_server_dir: /corral4/test/livedebug
galaxy_shed_tool_conf_file: /cvmfs/{{ galaxy_cvmfs_repo }}/config/shed_tool_conf.xml

#galaxy_venv_dir: defined in all.yml
galaxy_config_dir: "{{ galaxy_root }}/config"
galaxy_config_file: "{{ galaxy_config_dir }}/galaxy.yml"
galaxy_mutable_config_dir: "{{ galaxy_root }}/var"
galaxy_mutable_data_dir: "{{ galaxy_root }}/var"
# these don't need to be set if using the layout module, but usegalaxy_backup uses galaxyproject.galaxy's defaults
# without running the layout and breaks if they are unset
galaxy_cache_dir: "{{ galaxy_mutable_data_dir }}/cache"

galaxy_admin_email_to: galaxy-lab@bx.psu.edu




## used by: job_conf.yml template
galaxy_job_conf_pulsar_galaxy_url: "https://{{ galaxy_instance_hostname }}"
galaxy_job_conf_persistence_directory: "/srv/galaxy/{{ galaxy_instance_codename }}/var/pulsar_amqp_ack"
galaxy_job_conf_jetstream2_jobs_directory: "/jetstream2/scratch/{{ galaxy_instance_codename }}/jobs"

# These are automatically added to any runners that load galaxy.jobs.runners.pulsar:...
galaxy_job_conf_pulsar_runner_params:
  amqp_url: "{{ galaxy_job_conf_amqp_url }}"
  galaxy_url: "{{ galaxy_job_conf_pulsar_galaxy_url }}"
  persistence_directory: "{{ galaxy_job_conf_persistence_directory }}"
  amqp_acknowledge: "true"
  amqp_ack_republish_time: "1200"
  amqp_consumer_timeout: "2.0"
  amqp_publish_retry: "true"
  amqp_publish_retry_max_retries: "60"

galaxy_job_conf_runners:
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib64/libdrmaa.so
    invalidjobexception_retries: 5
    internalexception_retries: 5
  jetstream2:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: jetstream2
  stampede3:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: stampede3
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
  bridges:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: bridges
  expanse:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: expanse
  anvil:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: anvil
  rockfish_devgalaxy:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: rockfish_devgalaxy
    # FIXME: this doesn't work because status updates/finish go back to Main
    #amqp_url: "{{ galaxy_job_conf_main_amqp_url }}"
  jetstream2_k8s:
    load: galaxy.jobs.runners.pulsar:PulsarKubernetesJobRunner
    manager: jetstream2_k8s

galaxy_job_conf_default_container_id: /cvmfs/singularity.galaxyproject.org/all/python:3.8.3

galaxy_job_conf_singularity_volumes:
  local:
    - "$galaxy_root:ro"
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    # FIXME: this works with marius' containerized set-meta branch?
    #- "$storage"
    - "{{ galaxy_object_store_cache_path }}:ro"
    # what was this needed for?
    #- "{{ galaxy_new_file_path }}:ro"
    - "/corral4/{{ galaxy_instance_codename }}/files:ro"
    - "/corral4/{{ galaxy_instance_codename }}/files-test:ro"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/main.galaxyproject.org:ro"
    - "/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org:ro"
  jetstream:
    - "$galaxy_root:ro"
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/main.galaxyproject.org:ro"
    - "/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org:ro"
  bridges:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/data.galaxyproject.org:/cvmfs/data.galaxyproject.org:ro"
    - "/local:rw"
  expanse:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/data.galaxyproject.org:/cvmfs/data.galaxyproject.org:ro"
    - "/scratch:rw"
  anvil:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/tmp:rw"
  rockfish:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/tmp:rw"
  rockfish_devgalaxy:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/tmp:rw"
  # the tacc-apptainer module mostly automatically sets up mounts, but we need to ensure that /tmp inside the container
  # is always the host /tmp (e.g. in trinity containers it is /var/tmp)
  tacc_hpc:
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/tmp:/tmp:rw"
    - "/tmp:/var/tmp:rw"

galaxy_job_conf_tpv_config_files:
  - tools_pre_shared.yaml
  - https://raw.githubusercontent.com/galaxyproject/tpv-shared-database/fa63512571c1e4fbe54f62a5ffc03fae3f01df7c/tools.yml
  - defaults.yaml
  - environments.yaml
  - tools.yaml
  - users.yaml
  - roles.yaml

# explicit mappings in the "tools" section of the job_conf
galaxy_job_conf_tools:
  # explicit GxIT dispatching
  - id: interactive_tool_jupyter_notebook
    environment: tpv_dispatcher
    #environment: jetstream2_gxit
  #- id: interactive_tool_rstudio
  #  environment: tacc_k8s
  #- id: interactive_tool_panoply
  #  environment: tacc_k8s

galaxy_job_conf_limits:
  # this is a failsafe more than anything - actual limits are enforced on the environments
  - type: registered_user_concurrent_jobs
    value: 12
  - type: anonymous_user_concurrent_jobs
    value: 1

  # these probably don't really work
  - type: walltime
    value: '194:00:00'
  - type: output_size
    value: 50G

  # per-environments per-user limits
  - type: environment_user_concurrent_jobs
    id: roundup
    value: 8
  - type: environment_user_concurrent_jobs
    id: jetstream2
    value: 6
  - type: environment_user_concurrent_jobs
    id: tacc_k8s
    value: 1

  # tpv per-environment total limits
  - type: environment_total_concurrent_jobs
    id: bridges2
    value: 4
  - type: environment_total_concurrent_jobs
    id: expanse
    value: 4
  - type: environment_total_concurrent_jobs
    id: anvil
    value: 4
  - type: environment_total_concurrent_jobs
    id: rockfish
    value: 4
  - type: environment_total_concurrent_jobs
    id: rockfish_devgalaxy
    # actually controlled via TPV rules so this is more of a failsafe
    value: 4
  - type: environment_total_concurrent_jobs
    id: frontera
    value: 4
  - type: environment_total_concurrent_jobs
    id: frontera_rtx
    value: 4
  - type: environment_total_concurrent_jobs
    id: stampede3_skx
    value: 4
  - type: environment_total_concurrent_jobs
    id: stampede3_icx
    value: 4
  - type: environment_total_concurrent_jobs
    id: stampede3_spr
    value: 4

#pulsar_default_file_action: remote_transfer_tus
pulsar_default_file_action: remote_transfer

# Disable handlers in role, use playbook handlers
galaxy_systemd_mode: null

## used by: usegalaxy_privileged
galaxy_pgcleanup_actions:
  - delete_userless_histories
  - delete_exported_histories
  #- purge_deleted_users
  - purge_deleted_users_gdpr
  - purge_deleted_histories
  - purge_deleted_hdas
  - purge_historyless_hdas
  - purge_error_hdas
  - purge_hdas_of_purged_histories
  - delete_datasets
  - purge_datasets
galaxy_pgcleanup_days: 30
galaxy_pgcleanup_old_hdas:
  - object_store_id: corral-scratch
    days: 30

## used by: usegalaxy_admin
galaxy_log_archive_dir: /corral4/{{ galaxy_instance_codename }}/backup/log


## used by: galaxyproject.tools
galaxy_instance_url: "{{ galaxy_instance_hostname }}"
tool_list_file: files/galaxy/test.galaxyproject.org/tool_list.yaml


# galaxy_config hash moved to group_vars/all/galaxy_config_vars.yml

# FIXME: fails with Singularity volumes because this path does not exist on compute
#galaxy_local_tools_dir: "{{ galaxy_root }}/tools"
galaxy_local_tools_dir: "/corral4/gxsrc/{{ galaxy_instance_codename }}/local_tools"
galaxy_local_tools:
  - echo.xml
  - verkko.xml
  #- section_name: Interactive Tools
  #  file: interactivetool_rstudio.xml
  - section_name: Interactive Tools
    file: interactivetool_rstudio_bioc_3.20.xml
  #- section_name: Proteomics
  #  file: colabfold_msa.xml
  #- section_name: Proteomics
  #  file: colabfold_alphafold.xml


# TPV

tpv_mutable_dir: "{{ galaxy_root }}/tpv_staging"
tpv_config_dir_name: "tpv"
# the role defauls set this, but we need it before the role is imported
tpv_config_dir: "{{ galaxy_config_dir }}/{{ tpv_config_dir_name }}"
tpv_configs: "{{ galaxy_job_conf_tpv_config_files | reject('match', '.*://.*') | map('regex_replace', '^', 'templates/galaxy/config/tpv/') | map('regex_replace', '$', '.j2') }}"

# specifies config files to copy from the playbook
galaxy_config_files:
  - src: files/galaxy/config/panel_views/
    dest: "{{ galaxy_config.galaxy.panel_views_dir }}"
  - src: files/galaxy/config/tool_data_table_conf.xml
    dest: "{{ galaxy_config_dir }}/tool_data_table_conf.xml"
  - src: files/galaxy/config/tool_conf.xml
    dest: "{{ galaxy_config_dir }}/tool_conf.xml"
  - src: files/galaxy/config/data_manager_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['data_manager_config_file'] }}"
  - src: files/galaxy/config/tool_sheds_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['tool_sheds_config_file'] }}"
  - src: files/galaxy/config/job_metrics_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['job_metrics_config_file'] }}"
  - src: files/galaxy/config/job_resource_params_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['job_resource_params_file'] }}"
  - src: files/galaxy/config/dependency_resolvers_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['dependency_resolvers_config_file'] }}"
  - src: files/galaxy/config/container_resolvers_conf.yml
    dest: "{{ galaxy_config[galaxy_app_config_section]['container_resolvers_config_file'] }}"
  - src: files/galaxy/config/disposable_email_blocklist.conf
    dest: "{{ galaxy_config[galaxy_app_config_section]['email_domain_blocklist_file'] }}"
  - src: files/galaxy/config/workflow_schedulers_conf.xml
    dest: "{{ galaxy_config_dir }}/workflow_schedulers_conf.xml"
  - src: files/galaxy/config/user_preferences.yml
    dest: "{{ galaxy_config[galaxy_app_config_section]['user_preferences_extra_conf_path'] }}"
  - src: files/galaxy/config/trs_servers_conf.yml
    dest: "{{ galaxy_config_dir }}/trs_servers_conf.yml"

# specifies config files to template from the playbook
galaxy_config_templates:
  - src: templates/galaxy/config/job_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/job_conf.yml"
  - src: templates/galaxy/config/build_sites.yml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['build_sites_config_file'] }}"
  - src: templates/galaxy/config/oidc_config.xml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['oidc_config_file'] }}"
  - src: templates/galaxy/config/oidc_backends_config.xml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['oidc_backends_config_file'] }}"
  - src: templates/galaxy/config/tacc_k8s_pulsar_app_config.yml.j2
    dest: "{{ galaxy_config_dir }}/tacc_k8s_pulsar_app_config.yml"
  - src: templates/galaxy/config/file_sources_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/file_sources_conf.yml"
  - src: templates/galaxy/config/vault_conf.yml.j2
    dest: "{{ galaxy_config.galaxy.vault_config_file }}"
  - src: templates/galaxy/config/file_source_templates.yml.j2
    dest: "{{ galaxy_config_dir }}/file_source_templates.yml"
