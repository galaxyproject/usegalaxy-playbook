---

## these vars are defined in vault.yml
#
# used by: galaxyproject.galaxy (templating job_conf.yml)
galaxy_job_conf_amqp_url: "amqp://{{ pulsar_amqp_credentials }}@{{ amqp_server }}:5671//main_pulsar?ssl=1"

# used by: oidc backends config template
galaxy_google_client_id: "{{ vault_galaxy_google_client_id }}"
galaxy_google_client_secret: "{{ vault_galaxy_google_client_secret }}"

# used by: object store config template
galaxy_minio_idc_access_key: "idc"
galaxy_minio_idc_secret_key: "{{ vault_galaxy_minio_idc_secret_key }}"

# file_sources_conf.yml
covid_crg_ftp_staging_user: "{{ vault_covid_crg_ftp_staging_user }}"
covid_crg_ftp_staging_passwd: "{{ vault_covid_crg_ftp_staging_passwd }}"
genomeark_galaxy_aws_secret_access_key: "{{ vault_genomeark_galaxy_aws_secret_access_key }}"
genomeark_galaxy_aws_access_key_id: "{{ vault_genomeark_galaxy_aws_access_key_id }}"
genomeark_vgl_aws_secret_access_key: "{{ vault_genomeark_vgl_aws_secret_access_key }}"
genomeark_vgl_aws_access_key_id: "{{ vault_genomeark_vgl_aws_access_key_id }}"
paratus_aws_secret_access_key: "{{ vault_paratus_aws_secret_access_key }}"
paratus_aws_access_key_id: "{{ vault_paratus_aws_access_key_id }}"
elementbio_aws_secret_access_key: "{{ vault_elementbio_aws_secret_access_key }}"
elementbio_aws_access_key_id: "{{ vault_elementbio_aws_access_key_id }}"

# used by interactive_tool_chat_analysis
groq_api_key: "{{ vault_groq_api_key }}"

# Hashicorp Vault token
galaxy_vault_token: "{{ vault_galaxy_vault_token }}"

webhook_links:
  - gtn

webhook_plugins:
  - toolmsg
  - subdomain_switcher

# FIXME: "{{ galaxy_remote_users.privsep | default(omit) }}" is blanking remote_user on the play. I set the below option
# to workaround but this needs to be fixed in galaxyproject.galaxy, nothing should be breaking when privsep mode is not
# enabled in the role
galaxy_remote_users:
  privsep: "{{ galaxy_privileged_user }}"
  errdocs: "{{ galaxy_privileged_user }}"
  galaxy: "{{ galaxy_user }}"
  #root: gxadm

# FIXME: Same thing, the become log checks if galaxy_become_users.privsep is set and yet in the defaults we cause it to
# be always set
galaxy_become_users: {}

galaxy_server_dir: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
galaxy_shed_tools_dir: /cvmfs/{{ galaxy_cvmfs_repo }}/shed_tools
galaxy_shed_tool_conf_file: /cvmfs/{{ galaxy_cvmfs_repo }}/config/shed_tool_conf.xml

#galaxy_venv_dir defined in all.yml
galaxy_config_dir: "{{ galaxy_root }}/config"
galaxy_config_file: "{{ galaxy_config_dir }}/galaxy.yml"
galaxy_mutable_data_dir: "{{ galaxy_root }}/var"
galaxy_mutable_config_dir: "/corral4/{{ galaxy_instance_codename }}/config"
# these don't need to be set if the galaxyproject.galaxy layout tasks run, but usegalaxy_backup uses
# galaxyproject.galaxy's defaults without running the layout tasks and breaks if they are unset
galaxy_cache_dir: "{{ galaxy_mutable_data_dir }}/cache"

galaxy_admin_email_to: galaxy-lab@bx.psu.edu


## used by: job_conf.yml template
galaxy_job_conf_pulsar_galaxy_url: "https://staging.usegalaxy.org"
galaxy_job_conf_persistence_directory: "/corral4/{{ galaxy_instance_codename }}/pulsar_amqp_ack"
galaxy_job_conf_jetstream2_jobs_directory: "/jetstream2/scratch/{{ galaxy_instance_codename }}/jobs"

galaxy_systemd_memory_limit:
  # FIXME: this shouldn't be mule anymore. also "G" is hardcoded, but % would be nice.
  mule: 12

# These are automatically added to any runners that load galaxy.jobs.runners.pulsar:...
galaxy_job_conf_pulsar_runner_params:
  amqp_url: "{{ galaxy_job_conf_amqp_url }}"
  galaxy_url: "{{ galaxy_job_conf_pulsar_galaxy_url }}"
  persistence_directory: "{{ galaxy_job_conf_persistence_directory }}"
  amqp_acknowledge: "true"
  amqp_ack_republish_time: "1200"
  amqp_consumer_timeout: "2.0"
  amqp_publish_retry: "true"
  amqp_publish_retry_max_retries: "60"

galaxy_job_conf_runners:
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib64/libdrmaa.so
    invalidjobexception_retries: 5
    internalexception_retries: 5
  jetstream2:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: jetstream2
  stampede3:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: stampede3
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
  bridges:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: bridges
  expanse:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: expanse
  anvil:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: anvil
  rockfish_devgalaxy:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: rockfish_devgalaxy

galaxy_job_conf_tpv_config_files:
  - tools_pre_shared.yaml
  - https://raw.githubusercontent.com/galaxyproject/tpv-shared-database/fa63512571c1e4fbe54f62a5ffc03fae3f01df7c/tools.yml
  - defaults.yaml
  - environments.yaml
  - tools.yaml
  - users.yaml
  - roles.yaml

galaxy_job_conf_default_container_id: /cvmfs/singularity.galaxyproject.org/all/python:3.8.3

galaxy_job_conf_singularity_volumes:
  local:
    - "$galaxy_root:ro"
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "{{ galaxy_object_store_cache_path }}:ro"
    # what was this needed for?
    #- "{{ galaxy_new_file_path }}:ro"
    - "/corral4/{{ galaxy_instance_codename }}/objects:ro"
    - "/corral4/{{ galaxy_instance_codename }}/files:ro"
    - "/corral4/{{ galaxy_instance_codename }}/psufiles:ro"
    - "/corral4/data:ro"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/brc.galaxyproject.org:ro"
    - "/cvmfs/vgp.galaxyproject.org:ro"
    - "/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org:ro"
  jetstream:
    - "$galaxy_root:ro"
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/brc.galaxyproject.org:ro"
    - "/cvmfs/vgp.galaxyproject.org:ro"
    - "/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org:ro"
    - "/dev/shm:rw"
    - "/tmp/slurm_job_$SLURM_JOB_ID:rw"
  bridges:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/data.galaxyproject.org:/cvmfs/data.galaxyproject.org:ro"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/brc.galaxyproject.org:/cvmfs/brc.galaxyproject.org:ro"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/vgp.galaxyproject.org:/cvmfs/vgp.galaxyproject.org:ro"
    - "/local:rw"
  expanse:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/data.galaxyproject.org:/cvmfs/data.galaxyproject.org:ro"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/brc.galaxyproject.org:/cvmfs/brc.galaxyproject.org:ro"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/vgp.galaxyproject.org:/cvmfs/vgp.galaxyproject.org:ro"
    - "/scratch:rw"
  anvil:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/brc.galaxyproject.org:ro"
    - "/cvmfs/vgp.galaxyproject.org:ro"
    - "/tmp:rw"
  rockfish:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/brc.galaxyproject.org:ro"
    - "/cvmfs/vgp.galaxyproject.org:ro"
    - "/tmp:rw"
  rockfish_devgalaxy:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/tmp:rw"
  # the tacc-apptainer module mostly automatically sets up mounts, but we need to ensure that /tmp inside the container
  # is always the host /tmp (e.g. in trinity containers it is /var/tmp)
  tacc_hpc:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/brc.galaxyproject.org:ro"
    - "/cvmfs/vgp.galaxyproject.org:ro"
    - "/tmp:/tmp:rw"
    - "/tmp:/var/tmp:rw"

# TODO: this is the contents of wraptar, needs to be deployed somehow:
#
# #!/bin/bash
#
# wraptar_version='0.1.0'
#
# echo ''
# echo "Running ceph wraptar $wraptar_version around tar to suppress changed file warning" >&2
# echo ''
#
# if ! { stderr="$( { TAR_OPTIONS='--warning=no-file-changed' /usr/local/bin/tar "$@"; } 2>&1 1>&3 3>&- )"; } 3>&1; then
#     rc=$?
#     if [ -n "$stderr" ]; then
#         echo "$stderr" >&2
#         exit $rc
#     fi
# fi

# explicit mappings in the "tools" section of the job_conf
galaxy_job_conf_tools: []
  #- id: interactive_tool_jupyter_notebook
  #  handler: k8s
  #- id: interactive_tool_rstudio
  #  handler: k8s
  #- id: interactive_tool_panoply
  #  handler: k8s
  #- id: interactive_tool_phinch
  #  handler: k8s
  #- id: interactive_tool_pavian
  #  handler: k8s
  #- id: interactive_tool_qiskit_jupyter_notebook
  #  handler: k8s
  #- id: interactive_tool_blobtoolkit
  #  handler: k8s
  #- id: interactive_tool_phyloseq
  #  handler: k8s
  #- id: interactive_tool_chat_analysis
  #  handler: k8s

galaxy_job_conf_limits:
  # this is a failsafe more than anything - actual limits are enforced on the environments
  - type: registered_user_concurrent_jobs
    value: 8
  - type: anonymous_user_concurrent_jobs
    value: 1

  # these probably don't really work
  - type: walltime
    value: '194:00:00'
  - type: output_size
    value: 200G

  # pulsar envs
  - type: environment_user_concurrent_jobs
    id: jetstream2_k8s
    value: 1
  - type: environment_user_concurrent_jobs
    id: jetstream2_gxit
    value: 1

  # tpv per-environment user limits
  - type: environment_user_concurrent_jobs
    id: cyclone_conda_direct
    value: 4
  - type: environment_user_concurrent_jobs
    id: cyclone_small
    value: 6
  - type: environment_user_concurrent_jobs
    id: cyclone
    value: 4
  - type: environment_user_concurrent_jobs
    id: cyclone_mem
    value: 1
  - type: environment_user_concurrent_jobs
    id: jetstream2
    value: 4
  - type: environment_user_concurrent_jobs
    id: bridges2
    value: 2
  - type: environment_user_concurrent_jobs
    id: expanse
    value: 2
  - type: environment_user_concurrent_jobs
    id: rockfish
    value: 1
  - type: environment_total_concurrent_jobs
    id: rockfish_devgalaxy
    # actually controlled via TPV rules so this is more of a failsafe
    value: 4
  - type: environment_user_concurrent_jobs
    id: stampede3_skx
    value: 2
  - type: environment_user_concurrent_jobs
    id: stampede3_icx
    value: 2
  - type: environment_user_concurrent_jobs
    id: stampede3_spr
    value: 2

  # tpv per-environment total limits
  # NOTE: these limits are used in the TPV rank function and if they are changed here they must be updated there
  - type: environment_total_concurrent_jobs
    id: bridges2
    value: 100
  - type: environment_total_concurrent_jobs
    id: expanse
    # technically 64 for compute, 4096 for shared, but this prevents us from burning all of expanse at once
    value: 64
  - type: environment_total_concurrent_jobs
    id: anvil
    # limit is 6400 cores
    value: 100
  - type: environment_total_concurrent_jobs
    id: rockfish
    # doesn't have a published limit
    value: 50
  - type: environment_total_concurrent_jobs
    id: frontera
    value: 20
  - type: environment_total_concurrent_jobs
    id: stampede3_skx
    value: 20
  - type: environment_total_concurrent_jobs
    id: stampede3_icx
    value: 20
  - type: environment_total_concurrent_jobs
    id: stampede3_spr
    value: 20

#pulsar_default_file_action: remote_transfer_tus
pulsar_default_file_action: remote_transfer

# used in gravity config and nginx template
gxit_proxy_port: 8910

## used by: galaxyproject.galaxy
galaxy_errordocs_dest: "{{ nginx_srv }}/{{ galaxy_instance_hostname }}/error"
galaxy_errordocs_502_message: |
  You are seeing this message because a request to Galaxy timed out or was refused. This may be a temporary issue which
  could be resolved by retrying the operation you were performing. If you receive this message repeatedly or for an
  extended amount of time, please check for additional information on the
  <a href="https://status.galaxyproject.org/">Galaxy status page</a> or the
  <a href="https://mstdn.science/@galaxyproject">@galaxyproject Mastodon feed</a>. If the issue is not addressed on those
  sources, you may report it to the support team at
  <a href='mailto:galaxy-bugs@galaxyproject.org'>galaxy-bugs@galaxyproject.org</a>
  with details on what you were trying to do and the URL in the address bar.


## used by usegalaxy_privileged and templating job_conf.yml
galaxy_dynamic_rule_dir: "{{ galaxy_root }}/dynamic_rules"

galaxy_pgcleanup_actions:
  - delete_userless_histories
  - delete_exported_histories
  #- purge_deleted_users
  # this is less for GDPR purposes and more for allowing users to reregister with the same account later
  - purge_deleted_users_gdpr
  - purge_deleted_histories
  - purge_deleted_hdas
  - purge_historyless_hdas
  - purge_error_hdas
  - purge_hdas_of_purged_histories
  - delete_datasets
  - purge_datasets
galaxy_pgcleanup_days: 30
galaxy_pgcleanup_old_hdas:
  - object_store_id: corral4-scratch
    days: 30

## used by: usegalaxy_admin
galaxy_log_archive_dir: /corral4/{{ galaxy_instance_codename }}/backup/log

# We don't use galaxy_local_tools because we want to set a section
galaxy_local_tools_dir: "/corral4/gxsrc/{{ galaxy_instance_codename }}/local_tools"
galaxy_local_tools:
  # For reasons I wasn't able to get to the bottom of, when served from RStudio directly, the large static javascript
  # files sent by rserver stop sending prematurely and never complete on the client side, only when sent over tailscale,
  # but are fine when proxied with nginx.
  #- file: interactivetool_rstudio.xml
  #  section_name: Interactive Tools
  - file: interactivetool_phyloseq.xml
    section_name: Interactive Tools

# galaxy_config hash moved to group_vars/all/galaxy_config_vars.yml

tpv_mutable_dir: "{{ galaxy_root }}/tpv_staging"
tpv_config_dir_name: "tpv"
# the role defauls set this, but we need it before the role is imported
tpv_config_dir: "{{ galaxy_config_dir }}/{{ tpv_config_dir_name }}"
tpv_configs: "{{ galaxy_job_conf_tpv_config_files | reject('match', '.*://.*') | map('regex_replace', '^', 'templates/galaxy/config/tpv/') | map('regex_replace', '$', '.j2') }}"

# specifies config files to copy from the playbook
galaxy_config_files:
  - src: files/galaxy/config/panel_views/
    dest: "{{ galaxy_config.galaxy.panel_views_dir }}"
  - src: files/galaxy/config/tool_data_table_conf.xml
    dest: "{{ galaxy_config_dir }}/tool_data_table_conf.xml"
  - src: files/galaxy/config/tool_conf.xml
    dest: "{{ galaxy_config_dir }}/tool_conf.xml"
  - src: files/galaxy/config/data_manager_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['data_manager_config_file'] }}"
  - src: files/galaxy/config/tool_sheds_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['tool_sheds_config_file'] }}"
  - src: files/galaxy/config/disposable_email_blocklist.conf
    dest: "{{ galaxy_config[galaxy_app_config_section]['email_domain_blocklist_file'] }}"
  - src: files/galaxy/config/workflow_schedulers_conf.xml
    dest: "{{ galaxy_config_dir }}/workflow_schedulers_conf.xml"
  - src: files/galaxy/config/user_preferences.yml
    dest: "{{ galaxy_config[galaxy_app_config_section]['user_preferences_extra_conf_path'] }}"
  - src: files/galaxy/config/trs_servers_conf.yml
    dest: "{{ galaxy_config_dir }}/trs_servers_conf.yml"
  - src: files/galaxy/config/dropbox_file_source_template.yml
    dest: "{{ galaxy_config_dir }}/dropbox_file_source_template.yml"

# specifies config files to template from the playbook
# NOTE: changes here may need to be reflected in galaxy-vgp host_vars
galaxy_config_templates:
  - src: templates/galaxy/config/job_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/job_conf.yml"
  - src: templates/galaxy/config/build_sites.yml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['build_sites_config_file'] }}"
  - src: templates/galaxy/config/oidc_config.xml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['oidc_config_file'] }}"
  - src: templates/galaxy/config/oidc_backends_config.xml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['oidc_backends_config_file'] }}"
  - src: templates/galaxy/config/file_sources_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/file_sources_conf.yml"
  - src: templates/galaxy/config/vault_conf.yml.j2
    dest: "{{ galaxy_config.galaxy.vault_config_file }}"
  - src: templates/galaxy/config/file_source_templates.yml.j2
    dest: "{{ galaxy_config_dir }}/file_source_templates.yml"
