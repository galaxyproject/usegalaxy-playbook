---

global:
  default_inherits: _default

tools:
  _default:
    abstract: true
    cores: 1
    mem: cores * mem_factor
    context:
      force_cores: false
      cyclone_small_partition: normal
      cyclone_large_partition: multi
      cyclone_node: null
      js2_partition: tpv
      js2_gpu_partition: gpu-small
      mem_factor: 3.7
      # some rules use this as the maximum amount of allowed memory in scaled calculations
      max_scaled_mem: 190
      time: null
      default_time: "36:00:00"
      xdg_cache_home: null
    env:
    - execute: ulimit -c 0
    # tools in the shared DB override this, which then breaks some tools, so set on the destinations instead for
    # precendence
    #- name: _JAVA_OPTIONS
    #  value: -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR -Xmx{round(mem*0.9*1024)}m -Xms256m
    #- name: SINGULARITYENV__JAVA_OPTIONS
    #  value: $_JAVA_OPTIONS
    - name: HDF5_USE_FILE_LOCKING
      value: "FALSE"
    - name: SINGULARITYENV_HDF5_USE_FILE_LOCKING
      value: $HDF5_USE_FILE_LOCKING
    - name: LC_ALL
      value: C
    - name: SINGULARITYENV_LC_ALL
      value: {% raw %}'\\${{LANG:-C}}'
{% endraw %}
    - name: TERM
      value: vt100
    - name: SINGULARITYENV_TERM
      value: $TERM
    scheduling:
      accept:
      - pulsar
      require:
      - singularity
      reject:
      - offline
    #resubmit:
    #  drm_failure:
    #    condition: unknown_error and attempt <= 5
    #    destination: tpv_dispatcher
    rules:
    # NOTE: changes here need to be reflected in both rules and the _galaxy_lib abstract tool
    - id: tool_requires_local_conda
      if: tool.tool_type == "data_source"
      scheduling:
        require:
        - conda
        reject:
        - pulsar
        - singularity
    - id: tool_requires_local_galaxy
      if: tool.tool_type == "expression"
      # I swear this worked once in testing, but it seems not to now
      # TODO: retest once inheritance override is working
      #inherits: _galaxy_lib_local
      params:
        container_override:
        - type: singularity
          shell: /bin/sh
          identifier: /cvmfs/main.galaxyproject.org/singularity/rockylinux:9.5.20241118--0
      env:
      - name: SINGULARITYENV_PREPEND_PATH
        value: $GALAXY_VIRTUAL_ENV/bin
      - name: SINGULARITYENV_PYTHONPATH
        value: $GALAXY_LIB
      scheduling:
        reject:
        - pulsar
    - id: tool_requires_galaxy
      if: tool.tool_type not in ("data_source", "expression") and tool.requires_galaxy_python_environment
      #inherits: _galaxy_lib
      params:
        container_override:
        - type: singularity
          shell: /bin/sh
          identifier: /cvmfs/main.galaxyproject.org/singularity/rockylinux:9.5.20241118--0
      env:
      - name: SINGULARITYENV_PREPEND_PATH
        value: $GALAXY_VIRTUAL_ENV/bin
      - name: SINGULARITYENV_PYTHONPATH
        value: $GALAXY_LIB
    # NOTE: changes to training rules need to be reflected in roles
    - id: training_tag_small_rule
      if: |
        history = job.history
        tags = history and history.tags or []
        include_tags = []
        exclude_tags = ['training-exempt']
        any([(hta.user_value == 'training' or hta.user_tname == 'training') for hta in tags]) and helpers.tag_values_match(entity, include_tags, exclude_tags)
      context:
        time: "04:00:00"
        cyclone_small_partition: priority
        cyclone_large_partition: priority
        js2_partition: priority
      max_cores: 2
      max_mem: max_cores * 3.5
      scheduling:
        require:
        - training
        reject:
        - hpc
    - id: training_tag_large_rule
      if: |
        history = job.history
        tags = history and history.tags or []
        include_tags = ['training-large']
        exclude_tags = ['training-exempt']
        any([(hta.user_value == 'training' or hta.user_tname == 'training') for hta in tags]) and helpers.tag_values_match(entity, include_tags, exclude_tags)
      context:
        time: "04:00:00"
        cyclone_small_partition: priority
        cyclone_large_partition: priority
        js2_partition: priority
      max_cores: 8
      max_mem: max_cores * 3.5
      scheduling:
        require:
        - training
        reject:
        - hpc
    - id: training_tag_unclamped_rule
      if: |
        history = job.history
        tags = history and history.tags or []
        include_tags = ['training-exempt']
        exclude_tags = []
        any([(hta.user_value == 'training' or hta.user_tname == 'training') for hta in tags]) and helpers.tag_values_match(entity, include_tags, exclude_tags)
      context:
        time: "04:00:00"
        cyclone_small_partition: priority
        cyclone_large_partition: priority
        js2_partition: priority
      max_cores: 16
      scheduling:
        require:
        - training
        reject:
        - hpc
    # logs for every loop iteration for every concurrency limited job
    #- id: log
    #  if: true
    #  execute: log.debug(f"#### {tool.id=}, {tool.tool_type=}, {tool.requires_galaxy_python_environment=}")
    rank: |
      {{ tpv_python_blobs['rank'] | indent(width=6) }}
