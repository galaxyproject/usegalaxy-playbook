---

destinations:

  ###
  ### abstract envs for inheritance
  ###

  _default_environment:
    abstract: true
    env:
    # tools in the shared DB override this, which then breaks some tools, so set on the destinations instead for
    # precendence
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR -Xmx{round(mem*0.9*1024)}m -Xms256m
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS

  # CHANGES HERE MUST BE REFLECTED IN environments_vgp.yaml.j2
  _pulsar:
    abstract: true
    inherits: _default_environment
    params:
      tmp_dir: true
      outputs_to_working_directory: false
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      # TODO: pulsar side not deployed yet
      #live_tool_output_reporting: true
    scheduling:
      require:
      - pulsar

  _cyclone:
    abstract: true
    inherits: _default_environment
    runner: slurm
    #context:
    #  cyclone_*_partition and time are set on role and (default) tool entitites
    params:
      tmp_dir: true
      outputs_to_working_directory: true
      metadata_strategy: extended
    env:
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    scheduling:
      accept:
      - general
      - cvmfs
      - vgp
      prefer:
      - cyclone

  _cyclone_singularity:
    abstract: true
    inherits: _cyclone
    params:
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.local | join(',') }}"
      singularity_no_mount: null
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
    env:
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    scheduling:
      accept:
      - singularity
      # for TPV shared DB
      - docker

  # CHANGES HERE MUST BE REFLECTED IN environments_vgp.yaml.j2
  _jetstream2:
    abstract: true
    inherits: _pulsar
    runner: jetstream2
    #context:
    #  js2_partition and time are set on role and (default) tool entitites
    context:
      default_xdg_cache_home: "/tmp/{{ galaxy_user }}-cache"
    params:
      remote_metadata: true
      remote_property_galaxy_home: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
      jobs_directory: {{ galaxy_job_conf_jetstream2_jobs_directory }}
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.jetstream | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
      singularity_no_mount: null
    env:
    # SINGULARITYENV_TMP=$TMP and SINGULARITYENV_TMPDIR=$TMPDIR are set on the singularity command line
    - name: TMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TEMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TMPDIR
      value: $_GALAXY_JOB_TMP_DIR
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: XDG_CACHE_HOME
      value: "{xdg_cache_home or default_xdg_cache_home}"
    - name: TRINITY_SCRATCH_DIR
      value: /tmp/slurm_job_$SLURM_JOB_ID
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_XDG_CACHE_HOME
      value: $XDG_CACHE_HOME
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: SINGULARITY_CACHEDIR
      value: /jetstream2/scratch/{{ galaxy_instance_codename }}/singularity_cache
    - name: SINGULARITY_TMPDIR
      value: /jetstream2/scratch/{{ galaxy_instance_codename }}/singularity_tmp
    scheduling:
      accept:
      - general
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - jetstream2
      require:
      - pulsar
      #reject:
      #- offline

  _tacc_hpc:
    abstract: true
    inherits: _pulsar
    context:
      # override for max allowed time at TACC
      time: 24:00:00
    params:
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.tacc_hpc | join(',') }}"
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module unload xalt
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - name: TRINITY_SCRATCH_DIR
      value: /tmp
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR

  _stampede3:
    abstract: true
    inherits: _tacc_hpc
    params:
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org brc.galaxyproject.org vgp.galaxyproject.org singularity.galaxyproject.org -- apptainer
    env:
    {# FIXME: why is this not set in jobs? is it not being read from system profile or something? also, pretty sure it worked fine once without this in job 1464700 but not again? -#}
    - name: SCRATCH
      value: /scratch/03166/xcgalaxy
    - name: WORK2
      value: /work2/03166/xcgalaxy/stampede3
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    {# FIXME: why is this not set by module load tacc-apptainer? -#}
    - name: APPTAINER_CACHEDIR
      value: /work2/03166/xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"

  ###
  ### real envs
  ###

  #
  # cyclone
  #

  cyclone_conda_direct:
    inherits: _cyclone
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 4
    max_accepted_mem: 32
    params:
      native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time or default_time} --partition={cyclone_small_partition}{' -w ' + cyclone_node if cyclone_node else ''}"
    scheduling:
      accept:
      - training
      require:
      - conda

  cyclone:
    inherits: _cyclone_singularity
    min_accepted_cores: 3
    min_accepted_mem: 0
    max_accepted_cores: 12
    max_accepted_mem: 48
    params:
      native_specification: "--nodes=1 --ntasks={max(int(mem/4), 2)} --mem={round(mem*1024)} --time={time or default_time} --partition={cyclone_large_partition}{' -w ' + cyclone_node if cyclone_node else ''}"

  cyclone_small:
    inherits: _cyclone_singularity
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 2
    max_accepted_mem: 16
    params:
      native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time or default_time} --partition={cyclone_small_partition}{' -w ' + cyclone_node if cyclone_node else ''}"

  # this is only for pulsar-incompatible large jobs, so requires an explicit tag
  cyclone_mem:
    inherits: _cyclone_singularity
    min_accepted_cores: 1
    min_accepted_mem: 1
    max_accepted_cores: 32
    max_accepted_mem: 256
    max_cores: 8
    max_mem: 96
    params:
      native_specification: "--nodes=1 --ntasks={max(min(int(mem/4), 20), 1)} --mem={round(mem*1024)} --time={time or default_time} --partition={cyclone_large_partition}"
    scheduling:
      require:
      - cyclone-mem

  # *_training destinations are mainly for reporting, context is set in the role entity
  cyclone_training:
    inherits: cyclone
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 32
    max_accepted_mem: 64
    scheduling:
      require:
      - training
      accept:
      - cyclone-mem

  cyclone_small_training:
    inherits: cyclone_small
    scheduling:
      require:
      - training

  #
  # jetstream2
  #

  jetstream2:
    inherits: _jetstream2
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 64
    max_accepted_mem: 248
    params:
      submit_native_specification: "--nodes=1 --ntasks={max(int(mem/4), 1)} --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
    rules:
    - id: jetstream2_single_core_rule
      if: cores == 1
      params:
        submit_native_specification: "--nodes=1 --ntasks=1 --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
    - id: jetstream2_mem_rule
      # TODO: should maybe be less to cram more on Large Memory instances?
      if: not force_cores and cores > 1 and mem > 120
      params:
        submit_native_specification: "--nodes=1 --ntasks={int(mem/8)} --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
    - id: jetstream2_force_cores_rule
      if: force_cores and cores > 1
      params:
        submit_native_specification: "--nodes=1 --ntasks={int(cores)} --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
    #scheduling:
    #  reject:
    #  - offline

  jetstream2_lm:
    inherits: _jetstream2
    min_accepted_cores: 16
    min_accepted_mem: 248
    max_accepted_cores: 128
    max_accepted_mem: 980
    params:
      submit_native_specification: "--nodes=1 --ntasks={int(mem/8)} --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
    scheduling:
      require:
      - large-mem
      #reject:
      #- offline

  jetstream2_gpu:
    inherits: _jetstream2
    min_accepted_cores: 1
    min_accepted_mem: 0
    min_accepted_gpus: 0.2
    max_accepted_cores: 32
    max_accepted_gpus: 1
    max_accepted_mem: 125
    #context:
    #  js2_gpu_partition: gpu-small
    params:
      submit_native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time or default_time} --partition={js2_gpu_partition}"
      singularity_run_extra_arguments: --nv -B /etc/OpenCL/vendors:/usr/local/etc/OpenCL/vendors
    rules:
    #- id: offline
    #  if: true
    #  fail: GPU tools are unavailable until the Jetstream 2 maintenance is complete
    - id: jetstream2_gpu_medium_rule
      if: 0.2 < gpus <= 0.25
      context:
        js2_gpu_partition: gpu-medium
    - id: jetstream2_gpu_large_rule
      if: 0.25 < gpus <= 0.5
      context:
        js2_gpu_partition: gpu-large
    - id: jetstream2_gpu_large_rule
      if: 0.5 < gpus
      context:
        js2_gpu_partition: gpu-large
    env:
    # gmx_sim uses this, maybe others?
    - name: SINGULARITYENV_GPU_AVAILABLE
      value: "1"
    # ensures GPUs are visible inside container
    - execute: nvidia-modprobe -u -c=0
    scheduling:
      accept:
      - vgp
      require:
      - gpu
      #reject:
      #- offline

  jetstream2_resize_shm:
    inherits: _jetstream2
    params:
      submit_native_specification: "--nodes=1 --time={time or default_time} --partition=resize-shm"
      tmp_dir: false
      singularity_run_extra_arguments: -B /mnt/shm
    env:
    - name: TMPDIR
      value: /mnt/shm
    #rules:
    #  - id: offline
    #    if: true
    #    fail: NCBI FCS-GX is unavailable until the Jetstream 2 maintenance is complete
    scheduling:
      accept:
      - vgp
      require:
      - resize-shm
      - jetstream2
      #reject:
      #- offline

  jetstream2_gxit:
    runner: jetstream2
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 8
    max_accepted_mem: 32
    context:
      time: "06:00:00"
    params:
      submit_native_specification: "--partition=gxit --nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time or default_time}"
      tmp_dir: true
      outputs_to_working_directory: false
      singularity_enabled: false
      docker_enabled: true
      docker_set_user: null
      docker_memory: "{mem}G"
      container_resolvers:
      - type: explicit
      require_container: True
      container_monitor_command: /opt/galaxy-job-execution/bin/galaxy-container-monitor
      container_monitor_result: callback
      container_monitor_get_ip_method: command:/usr/bin/tailscale ip -4
      #remote_metadata: true
      #remote_property_galaxy_home: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
      remote_metadata: false
      metadata_strategy: directory_celery
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      #dependency_resolution: local
      dependency_resolution: remote
      rewrite_parameters: true
      jobs_directory: /jetstream2/scratch/{{ galaxy_instance_codename }}/jobs
    scheduling:
      accept:
      - pulsar
      - training
      require:
      - gxit
      #reject:
      #- offline

  jetstream2_training:
    inherits: jetstream2
    max_accepted_cores: 256
    max_accepted_mem: 4096
    max_cores: 64
    max_mem: 250
    scheduling:
      require:
      - training
      #reject:
      #- offline

  jetstream2_k8s:
    runner: jetstream2_k8s
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 32
    max_accepted_mem: 120
    params:
      docker_enabled: true  # probably shouldn't be needed but is still
      outputs_to_working_directory: false
      container_resolvers:
        - type: explicit
      docker_default_container_id: 'quay.io/biocontainers/python:3.6.7'
      #pulsar_container_image: '{{ pulsar_coexecution_container_image | default("galaxy/pulsar-pod-staging:0.15.0.1") }}'
      #k8s_namespace: ndc
      #k8s_walltime_limit: 86400  # 24 hours
      k8s_walltime_limit: 43200  # 12 hours
      pulsar_requests_cpu: 0.5
      pulsar_requests_memory: 0.5Gi
      pulsar_limits_cpu: 0.5
      pulsar_limits_memory: 0.5Gi
      tool_requests_cpu: 1.5
      tool_requests_memory: 2.5Gi
      tool_limits_cpu: 1.5
      tool_limits_memory: 2.5Gi
      #jobs_directory: /not/a/real/path
      pulsar_app_config_path: {{ galaxy_config_dir }}/tacc_k8s_pulsar_app_config.yml
      # Specify a non-default Pulsar staging container.
      # Generate job names with a string unique to this Galaxy (see
      # Kubernetes runner description).
      #k8s_galaxy_instance_id: mycoolgalaxy
      # Path to Kubernetes configuration fil (see Kubernetes runner description.)
      k8s_config_path: /home/{{ galaxy_user }}/.kube/config
    scheduling:
      accept:
      - pulsar
      - training
      - vgp
      require:
      - kubernetes
      #reject:
      #- offline

  #
  # hpc
  #

  bridges2:
    inherits: _pulsar
    runner: bridges
    min_accepted_cores: 16
    min_accepted_mem: 32
    #max_accepted_cores: 256
    #max_accepted_mem: 4096
    #max_cores: 128
    #max_mem: 250
    # Force large jobs to Stampede3 to preserve credits on shared resources
    max_accepted_cores: 64
    max_accepted_mem: 128
    context:
      time: 36:00:00
    params:
      # https://github.com/natefoo/slurm-drmaa/issues/83
      #submit_native_specification: "--partition=RM-shared --time={time or default_time} --nodes=1 --ntasks-per-node={int(mem/2)} --mem={int(mem/2)*2000}"
      submit_native_specification: "--partition=RM-shared --time={time or default_time} --nodes=1 --ntasks={int(mem/2)} --mem={int(mem/2)*2000}"
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      metadata_strategy: directory_celery
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.bridges | join(',') }}"
      singularity_no_mount: null
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org brc.galaxyproject.org vgp.galaxyproject.org singularity.galaxyproject.org -- singularity
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    rules:
    - if: mem >= 128
      params:
        submit_native_specification: "--partition=RM --time={time or default_time} --nodes=1 --ntasks={int(mem/2)}"
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    # RM-shared sets $SLURM_CPUS_ON_NODE == $SLURM_NTASKS == --ntasks
    # RM sets $SLURM_CPUS_ON_NODE to the full count (128) and $SLURM_NTASKS == --ntasks, if --ntasks is unset, $SLURM_NTASKS is unset
    # Overriding $GALAXY_SLOTS is probably only necessary if we actually determine that 128 is too many
    #- name: GALAXY_SLOTS
    #  value: "{min(int(mem/2), 64)}"
    - name: GALAXY_MEMORY_MB
      # TODO: use a scaling factor here?
      value: "{int(mem/2)*2000}"
    - name: TRINITY_SCRATCH_DIR
      value: $LOCAL
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: CVMFSEXEC_DIR
      value: $(dirname $_GALAXY_JOB_DIR)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo brc.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo vgp.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /ocean/projects/mcb140028p/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      - docker
      prefer:
      - bridges2
      require:
      - pulsar
      #reject:
      #- offline

  expanse:
    inherits: _pulsar
    runner: expanse
    min_accepted_cores: 8
    min_accepted_mem: 16
    max_accepted_cores: 64
    max_accepted_mem: 128
    context:
      time: 36:00:00
    params:
      # --mem={int(mem/2)*2048} ensures that mem is always exactly 2GB per core
      submit_native_specification: "--account=TG-MCB140147 --partition=shared --time={time or default_time} --nodes=1 --ntasks={int(mem/2)} --mem={int(mem/2)*2048}"
      jobs_directory: "{{ galaxy_job_conf_pulsar_jobs_dirs.expanse | default('/expanse/lustre/scratch/xgalaxy/temp_project/jobs/' + galaxy_instance_codename) }}"
      metadata_strategy: directory_celery
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.expanse | join(',') }}"
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org brc.galaxyproject.org vgp.galaxyproject.org singularity.galaxyproject.org oasis.opensciencegrid.org -- /cvmfs/oasis.opensciencegrid.org/mis/apptainer/bin/apptainer
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    rules:
    - if: mem >= 128
      params:
        submit_native_specification: "--account=TG-MCB140147 --partition=compute --time={time or default_time} --nodes=1 --ntasks={int(mem/2)}"
    env:
    # Expanse has some aggressive scratch cleanup, txn especially gets quickly nuked
    - execute: for d in $(seq 0 255 | while read n; do printf "%02x " "$n"; done) txn quarantaine; do mkdir -p /expanse/lustre/scratch/xgalaxy/temp_project/cvmfs_cache/$d; done
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - file: /etc/profile.d/00-sdsc-modules.sh
    - execute: module load slurm
    - execute: module load singularitypro
    # See comments on Bridges-2
    #- name: GALAXY_SLOTS
    #  value: "{int(cores)}"
    #  #value: "$SLURM_NTASKS"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - name: TRINITY_SCRATCH_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    # Expanse disabled user namespaces at some point
    #- name: CVMFSEXEC_PATH
    #  value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    #- execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    - name: CVMFSEXEC_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo brc.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo vgp.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /expanse/lustre/scratch/xgalaxy/temp_project/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - expanse
      require:
      - pulsar
      #reject:
      #- offline

  anvil:
    inherits: _pulsar
    runner: anvil
    min_accepted_cores: 8
    min_accepted_mem: 16
    max_accepted_cores: 64
    max_accepted_mem: 128
    params:
      submit_native_specification: "--partition=shared --nodes=1 --ntasks={int(mem/2)} --mem={int(mem/2)*2048} --time={time or default_time}"
      jobs_directory: /anvil/scratch/x-xcgalaxy/{{ galaxy_instance_codename }}/staging
      metadata_strategy: directory_celery
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org brc.galaxyproject.org vgp.galaxyproject.org singularity.galaxyproject.org -- apptainer
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.anvil | join(',') }}"
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    #- file: /etc/profile.d/z01_lmod.sh
    - execute: module --force purge
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - name: TRINITY_SCRATCH_DIR
      value: /tmp
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    - name: APPTAINER_CACHEDIR
      value: /anvil/scratch/x-xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - anvil
      require:
      - pulsar
      #reject:
      #- offline

  # could just dual-purpose devgalaxy Pulsar for a100 access
  #rockfish_gpu:
  #  runner: rockfish
  #  min_accepted_cores: 12
  #  min_accepted_mem: 48
  #  context:
  #    account: mschatz1_gpu
  #    partition: a100
  #    time: 24:00:00
  #  params:
  #    # https://github.com/natefoo/slurm-drmaa/issues/83
  #    #submit_native_specification: "--account={account} --partition={partition} --gres=gres:gpu:{gpus} --time={time} --nodes=1 --ntasks-per-node={int(cores)} --mem-per-cpu=4096"
  #    submit_native_specification: "--account={account} --partition={partition} --time={time} --nodes=1 --ntasks={int(cores)} --gres=gres:gpu:{gpus}"
  #    singularity_run_extra_arguments: --nv
  #    remote_metadata: false
  #    transport: curl
  #    default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
  #    dependency_resolution: local
  #    rewrite_parameters: true
  #    jobs_directory: /scratch4/nekrut/galaxy/{{ galaxy_instance_codename }}/staging
  #    singularity_enabled: true
  #    singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.rockfish | join(',') }}"
  #    singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- singularity
  #    singularity_no_mount: null
  #    container_resolvers:
  #    - type: explicit_singularity
  #    - type: cached_mulled_singularity
  #      cache_directory: /cvmfs/singularity.galaxyproject.org/all
  #      cache_directory_cacher_type: dir_mtime
  #    - type: mulled_singularity
  #    require_container: true
  #  env:
  #  - name: PATH
  #    value: /cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
  #  - name: GALAXY_SLOTS
  #    value: "{int(mem/4)}"
  #  #- name: GALAXY_MEMORY_MB
  #  #  value: "{mem*1024}"
  #  - execute: module load singularity/3.8.7
  #  - name: CVMFSEXEC_PATH
  #    value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
  #  - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
  #  # set in module
  #  #- name: SINGULARITY_CACHEDIR
  #  #  value: /scratch4/nekrut/galaxy/singularity_cache
  #  - name: SINGULARITY_PYTHREADS
  #    value: "9"
  #  scheduling:
  #    accept:
  #    - rockfish
  #    - hpc
  #    - cvmfs
  #    - singularity
  #    # for TPV shared DB
  #    - docker
  #    require:
  #    - pulsar
  #    - rockfish-gpu

  rockfish_devgalaxy:
    inherits: _default_environment
    runner: rockfish_devgalaxy
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 1
    max_accepted_mem: 4
    params:
      submit_native_specification: "--account=mschatz1 --partition=shared --time=12:00:00 --nodes=1 --ntasks={int(mem/4)} --mem={int(mem*1024)}"
      remote_metadata: false
      metadata_strategy: directory_celery
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch4/mschatz1/galaxy/rockfish-devgalaxy/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.rockfish_devgalaxy | join(',') }}"
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      require_container: true
      tmp_dir: true
      outputs_to_working_directory: false
    env:
    - name: PATH
      value: /cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{int(mem/4)}"
    - execute: module load singularity/3.8.7
    - name: SINGULARITY_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - singularity
      require:
      - rockfish-devgalaxy

  frontera:
    inherits: _tacc_hpc
    runner: frontera
    min_accepted_cores: 16
    min_accepted_mem: 96
    max_accepted_cores: 56
    max_accepted_mem: 190
    params:
      submit_native_specification: "--nodes=1 --ntasks={int(cores)} --ntasks-per-node={int(cores)} --time={time or default_time} --partition=small"
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org brc.galaxyproject.org vgp.galaxyproject.org singularity.galaxyproject.org -- apptainer
    env:
    - name: GALAXY_MEMORY_MB
      value: "190000"
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    {# FIXME: why is this not set by module load tacc-apptainer? -#}
    - name: APPTAINER_CACHEDIR
      value: /work2/03166/xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - frontera
      require:
      - pulsar
      reject:
      - offline

  frontera_rtx:
    inherits: _default_environment
    runner: frontera
    min_accepted_cores: 1
    min_accepted_mem: 2
    max_accepted_cores: 16
    max_accepted_mem: 120
    context:
      account: BIR23002
      partition: rtx
    params:
      submit_native_specification: "--account={account} --nodes=1 --ntasks={int(cores)} --time={time or default_time} --partition={partition}"
      singularity_run_extra_arguments: --nv
      # _tacc_hpc params
      outputs_to_working_directory: false
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      # the tacc-apptainer module automatically sets up mounts
      singularity_volumes: null
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: mulled_singularity
      require_container: true
    scheduling:
      accept:
      - cvmfs
      - singularity
      require:
      - frontera-rtx
      - pulsar
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"

  stampede3_skx:
    inherits: _stampede3
    runner: stampede3
    min_accepted_cores: 16
    min_accepted_mem: 128
    max_accepted_cores: 64
    max_accepted_mem: 190
    max_cores: 48
    max_mem: 184
    params:
      # $GALAXY_SLOTS is forced to 48 to avoid wastage
      #submit_native_specification: "--nodes=1 --ntasks={int(cores)} --ntasks-per-node={int(cores)} --time={time or default_time} --partition=skx"
      submit_native_specification: "--nodes=1 --time={time or default_time} --partition=skx"
      jobs_directory: /scratch/03166/xcgalaxy/jobs/{{ galaxy_instance_codename }}
    env:
    - name: GALAXY_SLOTS
      value: "48"
    - name: GALAXY_MEMORY_MB
      value: "188000"
    scheduling:
      accept:
      - stampede3
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - stampede3-skx
      require:
      - pulsar
      reject:
      - offline

  stampede3_icx:
    inherits: _stampede3
    runner: stampede3
    min_accepted_cores: 32
    min_accepted_mem: 190
    max_accepted_cores: 80
    max_accepted_mem: 4096
    max_cores: 80
    max_mem: 248
    params:
      # $GALAXY_SLOTS is forced to 64 to avoid wastage
      #submit_native_specification: "--nodes=1 --ntasks={int(cores)} --ntasks-per-node={int(cores)} --time={time or default_time} --partition=icx"
      submit_native_specification: "--nodes=1 --time={time or default_time} --partition=icx"
      jobs_directory: /scratch/03166/xcgalaxy/jobs/{{ galaxy_instance_codename }}
    env:
    - name: GALAXY_SLOTS
      value: "80"
    - name: GALAXY_MEMORY_MB
      value: "248000"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      - vgp
      prefer:
      - stampede3-icx
      require:
      - pulsar
      - stampede3
      #reject:
      #- offline

  stampede3_spr:
    inherits: _stampede3
    runner: stampede3
    min_accepted_cores: 112
    min_accepted_mem: 96
    max_accepted_cores: 256
    max_accepted_mem: 128
    max_cores: 112
    max_mem: 120
    params:
      # $GALAXY_SLOTS is forced to 112 to avoid wastage
      #submit_native_specification: "--nodes=1 --ntasks={int(cores)} --ntasks-per-node={int(cores)} --time={time or default_time} --partition=icx"
      submit_native_specification: "--nodes=1 --time={time or default_time} --partition=spr"
      jobs_directory: /scratch/03166/xcgalaxy/jobs/{{ galaxy_instance_codename }}
    env:
    - name: GALAXY_SLOTS
      value: "112"
    - name: GALAXY_MEMORY_MB
      value: "122880"
    scheduling:
      accept:
      - stampede3
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - stampede3-spr
      require:
      - pulsar
      reject:
      - offline
{% if galaxy_instance_codename == 'test' %}

  ls6:
    inherits: _tacc_hpc
    runner: ls6
    min_accepted_cores: 1
    min_accepted_mem: 0
    context:
      time: 48:00:00
    params:
      submit_native_specification: "--nodes=1 --ntasks={int(cores)} --time={time} --partition=normal"
      #submit_native_specification: "--nodes=1 --ntasks={int(cores)} --time=00:20:00 --partition=development"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org brc.galaxyproject.org vgp.galaxyproject.org singularity.galaxyproject.org -- apptainer
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.ls6 | join(',') }}"
    env:
    - name: GALAXY_MEMORY_MB
      value: "240000"
    # LS6 /scratch doesn't support cross-dir hard links
    - execute: CVMFSEXEC_DIR=$(mktemp -dt cvmfsexec-XXXXXX)
    - name: CVMFSEXEC_PATH
      value: $CVMFSEXEC_DIR/cvmfsexec
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    {# FIXME: why is this not set by module load tacc-apptainer? -#}
    - name: APPTAINER_CACHEDIR
      value: /work/03166/xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      require:
      - pulsar
      - ls6
      #reject:
      #- offline
{% endif %}
