---

destinations:

  _default_environment_vgp:
    abstract: true
    env:
    # tools in the shared DB override this, which then breaks some tools, so set on the destinations instead for
    # precendence
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR -Xmx{round(mem*0.9*1024)}m -Xms256m
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS

  jetstream2_vgp:
    # Cannot inherit a definition from another file
    #inherits: _jetstream2
    inherits: _default_environment_vgp
    runner: jetstream2
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 128
    max_accepted_mem: 1024
    max_cores: 128
    max_mem: 980
    #context:
    #  time: 36:00:00
    #   js2_partition is set on role and (default) tool entitites
    params:
      submit_native_specification: "--nodes=1 --ntasks={max(int(mem/4), 1)} --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      jobs_directory: {{ galaxy_job_conf_jetstream2_jobs_directory }}
      remote_metadata: true
      remote_property_galaxy_home: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.jetstream | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
      singularity_no_mount: null
      tmp_dir: true
      outputs_to_working_directory: false
      transport: curl
      dependency_resolution: local
      rewrite_parameters: true
    env:
    # SINGULARITYENV_TMP=$TMP and SINGULARITYENV_TMPDIR=$TMPDIR are set on the singularity command line
    - name: TMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TEMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TMPDIR
      value: $_GALAXY_JOB_TMP_DIR
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: XDG_CACHE_HOME
      value: /tmp/{{ galaxy_user }}-cache
    - name: TRINITY_SCRATCH_DIR
      value: /tmp/slurm_job_$SLURM_JOB_ID
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_XDG_CACHE_HOME
      value: $XDG_CACHE_HOME
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: SINGULARITY_CACHEDIR
      value: /jetstream2/scratch/{{ galaxy_instance_codename }}/singularity_cache
    - name: SINGULARITY_TMPDIR
      value: /jetstream2/scratch/{{ galaxy_instance_codename }}/singularity_tmp
    rules:
    - id: jetstream2_single_core_rule
      if: cores == 1
      params:
        submit_native_specification: "--nodes=1 --ntasks=1 --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
    - id: jetstream2_mem_rule
      if: not force_cores and cores > 1 and mem > 120
      params:
        submit_native_specification: "--nodes=1 --ntasks={int(mem/8)} --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
    - id: jetstream2_force_cores_rule
      if: force_cores and cores > 1
      params:
        submit_native_specification: "--nodes=1 --ntasks={int(cores)} --mem={int(mem*1024)} --time={time or default_time} --partition={js2_partition}"
    scheduling:
      accept:
      - general
      - cvmfs
      - docker
      prefer:
      - jetstream2
      require:
      - pulsar
      - singularity
      - vgp
      #reject:
      #- offline

  bridges2_em:
    # Cannot inherit a definition from another file
    #inherits: bridges2
    inherits: _default_environment_vgp
    runner: bridges
    min_accepted_cores: 24
    min_accepted_mem: 1024
    max_accepted_cores: 96
    max_accepted_mem: 4096
    context:
      time: 120:00:00
    params:
      submit_native_specification: "--partition=EM --time={time or default_time} --nodes=1 --ntasks={int(cores - (cores % 24))}"
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.bridges | join(',') }}"
      singularity_no_mount: null
      tmp_dir: true
      outputs_to_working_directory: false
      transport: curl
      dependency_resolution: local
      rewrite_parameters: true
      require_container: true
    env:
    # SINGULARITYENV_TMP=$TMP and SINGULARITYENV_TMPDIR=$TMPDIR are set on the singularity command line
    - name: TMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TEMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TMPDIR
      value: $_GALAXY_JOB_TMP_DIR
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: XDG_CACHE_HOME
      value: /tmp/{{ galaxy_user }}-cache
    - name: TRINITY_SCRATCH_DIR
      value: /tmp/slurm_job_$SLURM_JOB_ID
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_XDG_CACHE_HOME
      value: $XDG_CACHE_HOME
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: SINGULARITY_CACHEDIR
      value: /jetstream2/scratch/{{ galaxy_instance_codename }}/singularity_cache
    - name: SINGULARITY_TMPDIR
      value: /jetstream2/scratch/{{ galaxy_instance_codename }}/singularity_tmp

    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    # RM-shared sets $SLURM_CPUS_ON_NODE == $SLURM_NTASKS == --ntasks
    # RM sets $SLURM_CPUS_ON_NODE to the full count (128) and $SLURM_NTASKS == --ntasks, if --ntasks is unset, $SLURM_NTASKS is unset
    # Overriding $GALAXY_SLOTS is probably only necessary if we actually determine that 128 is too many
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - name: GALAXY_MEMORY_MB
      value: "{int(cores - (cores % 24))*42*1024}"
    - name: TRINITY_SCRATCH_DIR
      value: $LOCAL
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: CVMFSEXEC_DIR
      value: $(dirname $_GALAXY_JOB_DIR)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo brc.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo vgp.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /ocean/projects/mcb140028p/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      - docker
      - vgp
      require:
      - pulsar
      - bridges2-em
      #reject:
      #- offline

  stampede3_nvdimm:
    inherits: _default_environment_vgp
    runner: stampede3
    min_accepted_cores: 32
    min_accepted_mem: 960
    max_accepted_cores: 80
    max_accepted_mem: 4096
    max_cores: 80
    max_mem: 4096
    context:
      # override for max allowed time at TACC
      time: 48:00:00
    params:
      submit_native_specification: "--nodes=1 --time={time or default_time} --account=TG-MCB140147 --partition=nvdimm"
      tmp_dir: true
      outputs_to_working_directory: false
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.tacc_hpc | join(',') }}"
      singularity_no_mount: null
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org brc.galaxyproject.org vgp.galaxyproject.org singularity.galaxyproject.org -- apptainer
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module unload xalt
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - name: TRINITY_SCRATCH_DIR
      value: /tmp
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: SCRATCH
      value: /scratch/03166/xcgalaxy
    - name: WORK2
      value: /work2/03166/xcgalaxy/stampede3
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    - name: APPTAINER_CACHEDIR
      value: /work2/03166/xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"
    - name: GALAXY_SLOTS
      value: "80"
    - name: GALAXY_MEMORY_MB
      value: "4177920"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      - docker
      - vgp
      require:
      - pulsar
      - stampede3-nvdimm
      #reject:
      #- offline

